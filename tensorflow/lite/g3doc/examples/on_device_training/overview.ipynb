{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_nWetWWd_ns"
      },
      "source": [
        "##### Copyright 2024 The AI Edge Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "cellView": "form",
        "id": "2pHVBk_seED1"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7vSdG6sAIQn"
      },
      "source": [
        "# On-Device Training with TensorFlow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwc5GKHBASdc"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/examples/on_device_training/overview\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ee074e4"
      },
      "source": [
        "When deploying a TensorFlow Lite machine learning model on an edge device or mobile app, you may want to enable the model to be improved or personalized based on input from the device or end user. Using on-device training techniques allows you to update a model *without* data leaving your users' devices, improving user privacy, and without requiring users to update the device software.\n",
        "\n",
        "For example, you may have a model in your mobile app that recognizes fashion items, but you want users to get improved recognition performance over time based on their interests. Enabling on-device training allows users who are interested in shoes to get better at recognizing a particular style of shoe or shoe brand the more often they use your app.\n",
        "\n",
        "This tutorial shows you how to construct a TensorFlow Lite model that can be incrementally trained and improved within an installed Android app.\n",
        "\n",
        "Note: The on-device training technique can be added to existing TensorFlow Lite implementations, provided the devices you are targeting support local file storage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaWdLA3fQDK2"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This tutorial uses Python to train and convert a TensorFlow model before incorporating it into an Android app. Get started by installing and importing the following packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "9j4MGqyKQEo4",
        "outputId": "a665d19e-b9a4-4a3d-8225-1efeaec2c80c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.17.1\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOCyq3dTvfL6"
      },
      "source": [
        "Note: The On-Device Training APIs are available in TensorFlow version 2.7 and higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_omifE5JKpt4"
      },
      "source": [
        "## Classify images of clothing\n",
        "\n",
        "This example code uses the [Fashion MNIST dataset](https://keras.io/api/datasets/fashion_mnist/) to train a neural network model for classifying images of clothing. This dataset contains 60,000 small (28 x 28 pixel) grayscale images containing 10 different categories of fashion accessories, including dresses, shirts, and sandals.\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "       alt=\"Fashion MNIST images\">\n",
        "  <figcaption><b>Figure 1</b>: <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).</figcaption>\n",
        "</figure>\n",
        "\n",
        "You can explore this dataset in more depth in the [Keras classification tutorial](https://www.tensorflow.org/tutorials/keras/classification#import_the_fashion_mnist_dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN2N6hPEP-Ay"
      },
      "source": [
        "## Build a model for on-device training\n",
        "\n",
        "TensorFlow Lite models typically have only a single exposed function method (or [signature](https://www.tensorflow.org/lite/guide/signatures)) that allows you to call the model to run an inference. For a model to be trained and used on a device, you must be able to perform several separate operations, including train, infer, save, and restore functions for the model. You can enable this functionality by first extending your TensorFlow model to have multiple functions, and then exposing those functions as signatures when you convert your model to the TensorFlow Lite model format.\n",
        "\n",
        "The code example below shows you how to add the following functions to a TensorFlow model:\n",
        "\n",
        "*   `train` function trains the model with training data.\n",
        "*   `infer` function invokes the inference.\n",
        "*   `save` function saves the trainable weights into the file system.\n",
        "*   `restore` function loads the trainable weights from the file system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "d8577c80"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 28\n",
        "\n",
        "class Model(tf.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE), name='flatten'),\n",
        "        tf.keras.layers.Dense(128, activation='relu', name='dense_1'),\n",
        "        tf.keras.layers.Dense(10, name='dense_2')\n",
        "    ])\n",
        "\n",
        "    self.model.compile(\n",
        "        optimizer='sgd',\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "  # The `train` function takes a batch of input images and labels.\n",
        "  @tf.function(input_signature=[\n",
        "      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\n",
        "      tf.TensorSpec([None, 10], tf.float32),\n",
        "  ])\n",
        "  def train(self, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "      prediction = self.model(x)\n",
        "      loss = self.model.loss(y, prediction)\n",
        "    gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "    self.model.optimizer.apply_gradients(\n",
        "        zip(gradients, self.model.trainable_variables))\n",
        "    result = {\"loss\": loss}\n",
        "    return result\n",
        "\n",
        "  @tf.function(input_signature=[\n",
        "      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\n",
        "  ])\n",
        "  def infer(self, x):\n",
        "    logits = self.model(x)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"output\": probabilities,\n",
        "        \"logits\": logits\n",
        "    }\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def save(self, checkpoint_path):\n",
        "    tensor_names = [weight.name for weight in self.model.weights]\n",
        "    tensors_to_save = [weight.read_value() for weight in self.model.weights]\n",
        "    tf.raw_ops.Save(\n",
        "        filename=checkpoint_path, tensor_names=tensor_names,\n",
        "        data=tensors_to_save, name='save')\n",
        "    return {\n",
        "        \"checkpoint_path\": checkpoint_path\n",
        "    }\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def restore(self, checkpoint_path):\n",
        "    restored_tensors = {}\n",
        "    for var in self.model.weights:\n",
        "      restored = tf.raw_ops.Restore(\n",
        "          file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,\n",
        "          name='restore')\n",
        "      var.assign(restored)\n",
        "      restored_tensors[var.name] = restored\n",
        "    return restored_tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86tBMjoBnp6d"
      },
      "source": [
        "The `train` function in the code above uses the [GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) class to record operations for automatic differentiation. For more information on how to use this class, see the [Introduction to gradients and automatic differentiation](https://www.tensorflow.org/guide/autodiff).\n",
        "\n",
        "You could use the `Model.train_step` method of the keras model here instead of a from-scratch implementation. Just note that the loss (and metrics) returned by `Model.train_step` is the running average, and should be reset regularly (typically each epoch). See [Customize Model.fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit) for details.\n",
        "\n",
        "Note: The weights generated by this model are serialized into a TensorFlow 1 format checkpoint file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a813419961ef"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "Get the Fashion MNIST dataset for training your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "315b8b4dfc16"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eDn-bRD30sw"
      },
      "source": [
        "### Preprocess the dataset\n",
        "\n",
        "Pixel values in this dataset are between 0 and 255, and must be normalized to a value between 0 and 1 for processing by the model. Divide the values by 255 to make this adjustment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "g0FqHC0yCg6n"
      },
      "outputs": [],
      "source": [
        "train_images = (train_images / 255.0).astype(np.float32)\n",
        "test_images = (test_images / 255.0).astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bbee849ab73"
      },
      "source": [
        "Convert the data labels to categorical values by performing one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Fmc7EgYO30sw"
      },
      "outputs": [],
      "source": [
        "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79f5f372fb0e"
      },
      "source": [
        "Note: Make sure to preprocess your *training* and *testing* datasets in the same way so that your testing can accurately evaluate your model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkuDUFNNyAVN"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Before converting and setting up your TensorFlow Lite model, complete the initial training of your model using the preprocessed dataset and the `train` signature method. The following code runs model training for 100 epochs, processing batches of 100 images at a time, and displaying the loss value after every 10 epochs. Since this training run is processing quite a bit of data, it may take a few minutes to finish.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Diwn1MmkNVeX",
        "outputId": "6e09ec36-c6af-40db-9adc-160b01e92846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Binding inputs to tf.function failed due to `Can not cast TensorSpec(shape=(100, 10), dtype=tf.float64, name=None) to TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)`. Received args: (<tf.Tensor: shape=(100, 28, 28), dtype=float32, numpy=\narray([[[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       ...,\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>, <tf.Tensor: shape=(100, 10), dtype=float64, numpy=\narray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])>) and kwargs: {} for signature: (x: TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name=None), y: TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-40272013db70>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py\u001b[0m in \u001b[0;36mbind_function_inputs\u001b[0;34m(args, kwargs, function_type, default_values)\u001b[0m\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m     raise TypeError(\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;34mf\"Binding inputs to tf.function failed due to `{e}`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;34mf\"Received args: {args} and kwargs: {sanitized_kwargs} for signature:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Binding inputs to tf.function failed due to `Can not cast TensorSpec(shape=(100, 10), dtype=tf.float64, name=None) to TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)`. Received args: (<tf.Tensor: shape=(100, 28, 28), dtype=float32, numpy=\narray([[[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       ...,\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>, <tf.Tensor: shape=(100, 10), dtype=float64, numpy=\narray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])>) and kwargs: {} for signature: (x: TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name=None), y: TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))."
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 100\n",
        "epochs = np.arange(1, NUM_EPOCHS + 1, 1)\n",
        "losses = np.zeros([NUM_EPOCHS])\n",
        "m = Model()\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "train_ds = train_ds.batch(BATCH_SIZE)\n",
        "\n",
        "for i in range(NUM_EPOCHS):\n",
        "    for x, y in train_ds:\n",
        "        y = tf.cast(y, tf.float32)  # Cast labels to float32\n",
        "        result = m.train(x, y)\n",
        "\n",
        "    losses[i] = result['loss']\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"Finished {i+1} epochs\")\n",
        "        print(f\"  loss: {losses[i]:.3f}\")\n",
        "\n",
        "\n",
        "# Save the trained weights to a checkpoint.\n",
        "m.save('/tmp/model.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp2nkZj7rJXm"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, losses, label='Pre-training')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss [Cross Entropy]')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaMMDLLewAaX"
      },
      "source": [
        "Note: You should complete the initial training of your model before converting it to TensorFlow Lite format, so that the model has an initial set of weights, and is able to perform reasonable inferences *before* you start collecting data and conducting training runs on the device."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8YUTIvzMVw5"
      },
      "source": [
        "## Convert model to TensorFlow Lite format\n",
        "\n",
        "After you have extended your TensorFlow model to enable additional functions for on-device training and completed the initial training of the model, you can convert it to TensorFlow Lite format. The following code converts and saves your model to that format, including the set of signatures that you use with the TensorFlow Lite model on a device: `train, infer, save, restore`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwsDUEKFMYtq"
      },
      "outputs": [],
      "source": [
        "SAVED_MODEL_DIR = \"saved_model\"\n",
        "\n",
        "tf.saved_model.save(\n",
        "    m,\n",
        "    SAVED_MODEL_DIR,\n",
        "    signatures={\n",
        "        'train':\n",
        "            m.train.get_concrete_function(),\n",
        "        'infer':\n",
        "            m.infer.get_concrete_function(),\n",
        "        'save':\n",
        "            m.save.get_concrete_function(),\n",
        "        'restore':\n",
        "            m.restore.get_concrete_function(),\n",
        "    })\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
        "]\n",
        "converter.experimental_enable_resource_variables = True\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJSOtPISKSn2"
      },
      "source": [
        "### Setup the TensorFlow Lite signatures\n",
        "\n",
        "The TensorFlow Lite model you saved in the previous step contains several function signatures. You can access them through the `tf.lite.Interpreter` class and invoke each `restore`, `train`, `save`, and `infer` signature separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNX2vqXd2-HM"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "infer = interpreter.get_signature_runner(\"infer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTOM4alkteTO"
      },
      "source": [
        "Compare the output of the original model, and the converted lite model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDdaCmPEtE7P"
      },
      "outputs": [],
      "source": [
        "logits_original = m.infer(x=train_images[:1])['logits'][0]\n",
        "logits_lite = infer(x=train_images[:1])['logits'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpoZ1nTMKGEZ"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def compare_logits(logits):\n",
        "  width = 0.35\n",
        "  offset = width/2\n",
        "  assert len(logits)==2\n",
        "\n",
        "  keys = list(logits.keys())\n",
        "  plt.bar(x = np.arange(len(logits[keys[0]]))-offset,\n",
        "      height=logits[keys[0]], width=0.35, label=keys[0])\n",
        "  plt.bar(x = np.arange(len(logits[keys[1]]))+offset,\n",
        "      height=logits[keys[1]], width=0.35, label=keys[1])\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.ylabel('Logit')\n",
        "  plt.xlabel('ClassID')\n",
        "\n",
        "  delta = np.sum(np.abs(logits[keys[0]] - logits[keys[1]]))\n",
        "  plt.title(f\"Total difference: {delta:.3g}\")\n",
        "\n",
        "compare_logits({'Original': logits_original, 'Lite': logits_lite})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARUb37Hqa0Az"
      },
      "source": [
        "Above, you can see that the behavior of the model is not changed by the conversion to TFLite."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a5f73c8c5d6"
      },
      "source": [
        "## Retrain the model on a device\n",
        "\n",
        "After converting your model to TensorFlow Lite and deploying it with your app, you can retrain the model on a device using new data and the `train` signature method of your model. Each training run generates a new set of weights that you can save for re-use and further improvement of the model, as shown in the next section.\n",
        "\n",
        "Note: Since training tasks are resource intensive, you should consider performing them when users are not actively interacting with the device, and as a background process. Consider using the [WorkManager](https://developer.android.com/topic/libraries/architecture/workmanager) API to schedule model retraining as an asynchronous task.\n",
        "\n",
        "On Android, you can perform on-device training with TensorFlow Lite using either Java or C++ APIs. In Java, use the `Interpreter` class to load a model and drive model training tasks. The following example shows how to run the training procedure using the `runSignature` method:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvbqRxnNs4NG"
      },
      "source": [
        "```Java\n",
        "try (Interpreter interpreter = new Interpreter(modelBuffer)) {\n",
        "    int NUM_EPOCHS = 100;\n",
        "    int BATCH_SIZE = 100;\n",
        "    int IMG_HEIGHT = 28;\n",
        "    int IMG_WIDTH = 28;\n",
        "    int NUM_TRAININGS = 60000;\n",
        "    int NUM_BATCHES = NUM_TRAININGS / BATCH_SIZE;\n",
        "\n",
        "    List<FloatBuffer> trainImageBatches = new ArrayList<>(NUM_BATCHES);\n",
        "    List<FloatBuffer> trainLabelBatches = new ArrayList<>(NUM_BATCHES);\n",
        "\n",
        "    // Prepare training batches.\n",
        "    for (int i = 0; i < NUM_BATCHES; ++i) {\n",
        "        FloatBuffer trainImages = FloatBuffer.allocateDirect(BATCH_SIZE * IMG_HEIGHT * IMG_WIDTH).order(ByteOrder.nativeOrder());\n",
        "        FloatBuffer trainLabels = FloatBuffer.allocateDirect(BATCH_SIZE * 10).order(ByteOrder.nativeOrder());\n",
        "\n",
        "        // Fill the data values...\n",
        "        trainImageBatches.add(trainImages.rewind());\n",
        "        trainImageLabels.add(trainLabels.rewind());\n",
        "    }\n",
        "\n",
        "    // Run training for a few steps.\n",
        "    float[] losses = new float[NUM_EPOCHS];\n",
        "    for (int epoch = 0; epoch < NUM_EPOCHS; ++epoch) {\n",
        "        for (int batchIdx = 0; batchIdx < NUM_BATCHES; ++batchIdx) {\n",
        "            Map<String, Object> inputs = new HashMap<>();\n",
        "            inputs.put(\"x\", trainImageBatches.get(batchIdx));\n",
        "            inputs.put(\"y\", trainLabelBatches.get(batchIdx));\n",
        "\n",
        "            Map<String, Object> outputs = new HashMap<>();\n",
        "            FloatBuffer loss = FloatBuffer.allocate(1);\n",
        "            outputs.put(\"loss\", loss);\n",
        "\n",
        "            interpreter.runSignature(inputs, outputs, \"train\");\n",
        "\n",
        "            // Record the last loss.\n",
        "            if (batchIdx == NUM_BATCHES - 1) losses[epoch] = loss.get(0);\n",
        "        }\n",
        "\n",
        "        // Print the loss output for every 10 epochs.\n",
        "        if ((epoch + 1) % 10 == 0) {\n",
        "            System.out.println(\n",
        "              \"Finished \" + (epoch + 1) + \" epochs, current loss: \" + loss.get(0));\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // ...\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPEzyHAZs7Gl"
      },
      "source": [
        "You can see a complete code example of model retraining inside an Android app in the [model personalization demo app](https://github.com/tensorflow/examples/blob/master/lite/examples/model_personalization/android/transfer_api/src/main/java/org/tensorflow/lite/examples/transfer/api/LiteMultipleSignatureModel.java)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuBlL9z5GASQ"
      },
      "source": [
        "Run training for a few epochs to improve or personalize the model. In practice, you would run this additional training using data collected on the device. For simplicity, this example uses the same training data as the previous training step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjQ5xrhyGcIQ"
      },
      "outputs": [],
      "source": [
        "train = interpreter.get_signature_runner(\"train\")\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "BATCH_SIZE = 100\n",
        "more_epochs = np.arange(epochs[-1]+1, epochs[-1] + NUM_EPOCHS + 1, 1)\n",
        "more_losses = np.zeros([NUM_EPOCHS])\n",
        "\n",
        "\n",
        "for i in range(NUM_EPOCHS):\n",
        "  for x,y in train_ds:\n",
        "    result = train(x=x, y=y)\n",
        "  more_losses[i] = result['loss']\n",
        "  if (i + 1) % 10 == 0:\n",
        "    print(f\"Finished {i+1} epochs\")\n",
        "    print(f\"  loss: {more_losses[i]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX7dQXx_iPuv"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, losses, label='Pre-training')\n",
        "plt.plot(more_epochs, more_losses, label='On device')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss [Cross Entropy]')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbe9_LjAbEMF"
      },
      "source": [
        "Above you can see that the on-device training picks up exactly where the pretraining stopped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDIi0_RlPb2n"
      },
      "source": [
        "## Save the trained weights\n",
        "\n",
        "When you complete a training run on a device, the model updates the set of weights it is using in memory. Using the `save` signature method you created in your TensorFlow Lite model, you can save these weights to a checkpoint file for later reuse and improve your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c3d3cc5f171"
      },
      "outputs": [],
      "source": [
        "save = interpreter.get_signature_runner(\"save\")\n",
        "\n",
        "save(checkpoint_path=np.array(\"/tmp/model.ckpt\", dtype=np.string_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvlZN-rhR_Ag"
      },
      "source": [
        "In your Android application, you can store the generated weights as a checkpoint file in the internal storage space allocated for your app.\n",
        "\n",
        "```Java\n",
        "try (Interpreter interpreter = new Interpreter(modelBuffer)) {\n",
        "    // Conduct the training jobs.\n",
        "\n",
        "    // Export the trained weights as a checkpoint file.\n",
        "    File outputFile = new File(getFilesDir(), \"checkpoint.ckpt\");\n",
        "    Map<String, Object> inputs = new HashMap<>();\n",
        "    inputs.put(\"checkpoint_path\", outputFile.getAbsolutePath());\n",
        "    Map<String, Object> outputs = new HashMap<>();\n",
        "    interpreter.runSignature(inputs, outputs, \"save\");\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSDydMyOQfL5"
      },
      "source": [
        "## Restore the trained weights\n",
        "\n",
        "Any time you create an interpreter from a TFLite model, the interpreter will initially load the original model weights.\n",
        "\n",
        "So after you've done some training and saved a checkpoint file, you'll need to run the `restore` signature method to load the checkpoint.\n",
        "\n",
        "A good rule is \"Anytime you create an Interpreter for a model, if the checkpoint exists, load it\". If you need to reset the model to the baseline behavior, just delete the checkpoint and create a fresh interpreter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yIZoLveRZgp"
      },
      "outputs": [],
      "source": [
        "another_interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "another_interpreter.allocate_tensors()\n",
        "\n",
        "infer = another_interpreter.get_signature_runner(\"infer\")\n",
        "restore = another_interpreter.get_signature_runner(\"restore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjiUbx7zIoLq"
      },
      "outputs": [],
      "source": [
        "logits_before = infer(x=train_images[:1])['logits'][0]\n",
        "\n",
        "# Restore the trained weights from /tmp/model.ckpt\n",
        "restore(checkpoint_path=np.array(\"/tmp/model.ckpt\", dtype=np.string_))\n",
        "\n",
        "logits_after = infer(x=train_images[:1])['logits'][0]\n",
        "\n",
        "compare_logits({'Before': logits_before, 'After': logits_after})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7T6pja2bPqV"
      },
      "source": [
        "The checkpoint was generated by training and saving with TFLite. Above you can see that applying the checkpoint updates the behavior of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAo-3Eg7oGH7"
      },
      "source": [
        "Note: Loading the saved weights from the checkpoint can take time, based on the number of variables in the model and the size of the checkpoint file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9I_-gjdSnGn"
      },
      "source": [
        "In your Android app, you can restore the serialized, trained weights from the checkpoint file you stored earlier.\n",
        "\n",
        "```Java\n",
        "try (Interpreter anotherInterpreter = new Interpreter(modelBuffer)) {\n",
        "    // Load the trained weights from the checkpoint file.\n",
        "    File outputFile = new File(getFilesDir(), \"checkpoint.ckpt\");\n",
        "    Map<String, Object> inputs = new HashMap<>();\n",
        "    inputs.put(\"checkpoint_path\", outputFile.getAbsolutePath());\n",
        "    Map<String, Object> outputs = new HashMap<>();\n",
        "    anotherInterpreter.runSignature(inputs, outputs, \"restore\");\n",
        "}\n",
        "```\n",
        "\n",
        "Note: When your application restarts, you should reload your trained weights prior to running new inferences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjcrv57DSkz2"
      },
      "source": [
        "## Run Inference using trained weights\n",
        "\n",
        "Once you have loaded previously saved weights from a checkpoint file, running the `infer` method uses those weights with your original model to improve predictions. After loading the saved weights, you can use the `infer` signature method as shown below.\n",
        "\n",
        "Note: Loading the saved weights is not required to run an inference, but running in that configuration produces predictions using the originally trained model, without improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ROmlpHWS0nX"
      },
      "outputs": [],
      "source": [
        "infer = another_interpreter.get_signature_runner(\"infer\")\n",
        "result = infer(x=test_images)\n",
        "predictions = np.argmax(result[\"output\"], axis=1)\n",
        "\n",
        "true_labels = np.argmax(test_labels, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6nHopKlAD6-"
      },
      "outputs": [],
      "source": [
        "result['output'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGPtqeVULZui"
      },
      "source": [
        "Plot the predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHbRasdfasd4"
      },
      "outputs": [],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "def plot(images, predictions, true_labels):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for i in range(25):\n",
        "      plt.subplot(5,5,i+1)\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "      plt.grid(False)\n",
        "      plt.imshow(images[i], cmap=plt.cm.binary)\n",
        "      color = 'b' if predictions[i] == true_labels[i] else 'r'\n",
        "      plt.xlabel(class_names[predictions[i]], color=color)\n",
        "  plt.show()\n",
        "\n",
        "plot(test_images, predictions, true_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm8BxJOm_4rG"
      },
      "outputs": [],
      "source": [
        "predictions.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eijDL3jNS6WI"
      },
      "source": [
        "In your Android application, after restoring the trained weights, run the inferences based on the loaded data.\n",
        "\n",
        "```Java\n",
        "try (Interpreter anotherInterpreter = new Interpreter(modelBuffer)) {\n",
        "    // Restore the weights from the checkpoint file.\n",
        "\n",
        "    int NUM_TESTS = 10;\n",
        "    FloatBuffer testImages = FloatBuffer.allocateDirect(NUM_TESTS * 28 * 28).order(ByteOrder.nativeOrder());\n",
        "    FloatBuffer output = FloatBuffer.allocateDirect(NUM_TESTS * 10).order(ByteOrder.nativeOrder());\n",
        "\n",
        "    // Fill the test data.\n",
        "\n",
        "    // Run the inference.\n",
        "    Map<String, Object> inputs = new HashMap<>();\n",
        "    inputs.put(\"x\", testImages.rewind());\n",
        "    Map<String, Object> outputs = new HashMap<>();\n",
        "    outputs.put(\"output\", output);\n",
        "    anotherInterpreter.runSignature(inputs, outputs, \"infer\");\n",
        "    output.rewind();\n",
        "\n",
        "    // Process the result to get the final category values.\n",
        "    int[] testLabels = new int[NUM_TESTS];\n",
        "    for (int i = 0; i < NUM_TESTS; ++i) {\n",
        "        int index = 0;\n",
        "        for (int j = 1; j < 10; ++j) {\n",
        "            if (output.get(i * 10 + index) < output.get(i * 10 + j)) index = testLabels[j];\n",
        "        }\n",
        "        testLabels[i] = index;\n",
        "    }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cznoIDphGPEg"
      },
      "source": [
        "Congratulations! You now have built a TensorFlow Lite model that supports on-device training. For more coding details, check out the example implementation in the [model personalization demo app](https://github.com/tensorflow/examples/tree/master/lite/examples/model_personalization).\n",
        "\n",
        "If you are interested in learning more about image classification, check [Keras classification tutorial](https://www.tensorflow.org/tutorials/keras/classification) on the TensorFlow official guide page. This tutorial is based on that exercise and provides more depth on the subject of classification.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "overview.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}